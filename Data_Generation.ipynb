{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uXxNA_QltaF"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import scipy.io\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "ROOT_PATH = Path('/content/drive/MyDrive/Colab Notebooks/ECE 2500Y')\n",
        "\n",
        "dataset_A_path = ROOT_PATH / 'Dataset_A(days1_3).mat'\n",
        "dataset_A = scipy.io.loadmat(dataset_A_path)\n",
        "dataset_B_path = ROOT_PATH / 'Dataset_B(days1_3).mat'\n",
        "dataset_B = scipy.io.loadmat(dataset_B_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5gXdaQFm4_6"
      },
      "outputs": [],
      "source": [
        "participant_baseline_data_A = dataset_A.get('Participants', 'Unknown')\n",
        "participant_12_weeks_data_A = dataset_A.get('Participants_12weeks', 'Unknown')\n",
        "participant_baseline_data_B = dataset_B.get('Participants', 'Unknown')\n",
        "participant_12_weeks_data_B = dataset_B.get('Participants_12weeks', 'Unknown')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXAPHsCEBO_3"
      },
      "source": [
        "#Data generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NB1y7UzHCauB"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Helpers\n",
        "def extend_list(lst1, lst2):\n",
        "  for i in range(len(lst1)):\n",
        "    lst1[i].extend(lst2[i])\n",
        "  return lst1\n",
        "\n",
        "def get_one_day_trips(data, idx):\n",
        "  return [data[0][i][idx].shape[0] for i in range(len(data[0]))]\n",
        "\n",
        "def get_number_of_trips_in_three_days(data):\n",
        "  return [get_one_day_trips(data, i) for i in range(3)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGkBypQiPSdF"
      },
      "outputs": [],
      "source": [
        "def generate_frequency_based_categorical(num_samples, data):\n",
        "    unique, counts = np.unique(data, return_counts=True)\n",
        "    probabilities = counts / counts.sum()\n",
        "    #print(f\"Unique: {unique}, Counts: {counts}, Probabilities: {probabilities}\")\n",
        "\n",
        "    return np.random.choice(unique, num_samples, p=probabilities)\n",
        "\n",
        "def get_col(data, col_idx):\n",
        "  col_lst = [data[0][i][k][m][col_idx] for i in range(len(data[0])) for k in range(len(data[0][i])) for m in range(len(data[0][i][k]))]\n",
        "  return col_lst\n",
        "\n",
        "def is_night(time_lst):\n",
        "  return [1 if (0 < itm < 0.25) or (1 < itm < 1.25) else 0 for itm in time_lst]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCD4kL9SPfKc"
      },
      "outputs": [],
      "source": [
        "from functools import partial\n",
        "\n",
        "\n",
        "# Datasets\n",
        "baselines = [participant_baseline_data_A, participant_baseline_data_B]\n",
        "twleve_weeks = [participant_12_weeks_data_A, participant_12_weeks_data_B]\n",
        "\n",
        "def generate_daily_num_of_trips(data):\n",
        "  day1_trips = generate_frequency_based_categorical(1, data[0])[0]\n",
        "  day2_trips = generate_frequency_based_categorical(1, data[1])[0]\n",
        "  day3_trips = generate_frequency_based_categorical(1, data[2])[0]\n",
        "  return day1_trips, day2_trips, day3_trips\n",
        "\n",
        "def generate_one_paitient(which_dataset):\n",
        "  partial_get_cols = list(map(lambda dataset: partial(get_col, dataset), which_dataset))\n",
        "\n",
        "  col_stacks = [[] for _ in range(6)]\n",
        "  for partial_get_col in partial_get_cols:\n",
        "    col_stacks = extend_list(col_stacks, list(map(partial_get_col, range(6))))\n",
        "\n",
        "  tep = extend_list(get_number_of_trips_in_three_days(which_dataset[0]), get_number_of_trips_in_three_days(which_dataset[1]))\n",
        "  day1_num_of_trips, day2_num_of_trips, day3_num_of_trips = generate_daily_num_of_trips(tep)\n",
        "\n",
        "  day1_2_3_time_lsts = list(map(sorted, list(map(lambda x: generate_frequency_based_categorical(x, col_stacks[0]), [day1_num_of_trips, day2_num_of_trips, day3_num_of_trips]))))\n",
        "\n",
        "  col_stacks_except_time_and_night_day1 = list(map(lambda x: generate_frequency_based_categorical(day1_num_of_trips, x), col_stacks[1:5]))\n",
        "  col_stacks_except_time_and_night_day2 = list(map(lambda x: generate_frequency_based_categorical(day2_num_of_trips, x), col_stacks[1:5]))\n",
        "  col_stacks_except_time_and_night_day3 = list(map(lambda x: generate_frequency_based_categorical(day3_num_of_trips, x), col_stacks[1:5]))\n",
        "  col_stacks_except_time_and_night = [col_stacks_except_time_and_night_day1, col_stacks_except_time_and_night_day2, col_stacks_except_time_and_night_day3]\n",
        "\n",
        "  is_nights = list(map(is_night, day1_2_3_time_lsts))\n",
        "  col_stacks_generated_data = []\n",
        "  for i in range(len(day1_2_3_time_lsts)):\n",
        "    lst = [day1_2_3_time_lsts[i]] +  col_stacks_except_time_and_night[i] + [is_nights[i]]\n",
        "    col_stack_one_day = np.column_stack(lst)\n",
        "    col_stacks_generated_data.append(col_stack_one_day)\n",
        "\n",
        "\n",
        "\n",
        "  #print(col_stacks_generated_data)\n",
        "  return col_stacks_generated_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "usweLnZvdg3G"
      },
      "outputs": [],
      "source": [
        "n = 10000 # Generate 1000 patients, 3 days data each\n",
        "\n",
        "baseline_gen_data = [generate_one_paitient(baselines) for _ in range(n)]\n",
        "twleve_weeks_gen_data = [generate_one_paitient(twleve_weeks) for _ in range(n)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7pQiRACBLp8"
      },
      "source": [
        "#Data prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlD98GfHKtPd"
      },
      "source": [
        "##Actual Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWLnVbWTKsrc"
      },
      "outputs": [],
      "source": [
        "def stack_participant_data(x):\n",
        "  result_array = []\n",
        "  for i in range(len(x[0])):\n",
        "    combined_array = np.vstack((x[0][i][0],x[0][i][1],x[0][i][2]))\n",
        "    transposed_array = combined_array.T\n",
        "\n",
        "    new_array = np.copy(transposed_array[0])\n",
        "    counter = 0\n",
        "    for i in range(1, len(transposed_array[0])):\n",
        "        if transposed_array[0][i] < transposed_array[0][i - 1] and counter < 3:\n",
        "            new_array[i:] += 1\n",
        "            counter += 1\n",
        "    transposed_array[0] = new_array\n",
        "    transposed_array = transposed_array[[0, 2, 3, 5]]\n",
        "    result_array.append(transposed_array)\n",
        "\n",
        "  return result_array\n",
        "\n",
        "stacked_participant_baseline = stack_participant_data(participant_baseline_data_A) + stack_participant_data(participant_baseline_data_B)\n",
        "stacked_participant_week12 = stack_participant_data(participant_12_weeks_data_A) + stack_participant_data(participant_12_weeks_data_B)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfyVHlZ2KvTB"
      },
      "source": [
        "##Generated data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "puwvZdN0LTE1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def select_and_transpose_columns(data, columns_to_remove):\n",
        "    processed_data = []\n",
        "\n",
        "    for inner_list in data:\n",
        "        processed_inner_list = []\n",
        "        for array in inner_list:\n",
        "            all_columns = np.arange(array.shape[1])\n",
        "            columns_to_keep = np.setdiff1d(all_columns, columns_to_remove)\n",
        "            selected_columns = array[:, columns_to_keep]\n",
        "            transposed_array = selected_columns.T\n",
        "            processed_inner_list.append(transposed_array)\n",
        "        processed_data.append(processed_inner_list) #这里我先不rag\n",
        "\n",
        "    return processed_data\n",
        "\n",
        "columns = [0, 1, 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8fQYTiNOCbgN"
      },
      "outputs": [],
      "source": [
        "def sum_of_specific_column(data, column_index):\n",
        "\n",
        "    sums_of_columns = []\n",
        "\n",
        "    for inner_list in data:\n",
        "        inner_list_sum = 0\n",
        "        for array in inner_list:\n",
        "            inner_list_sum += np.sum(array[:, column_index])\n",
        "        sums_of_columns.append(inner_list_sum/3)\n",
        "\n",
        "    return sums_of_columns\n",
        "\n",
        "baseline_gen_trip_label = sum_of_specific_column(baseline_gen_data, 1)\n",
        "twleve_weeks_gen_trip_label = sum_of_specific_column(twleve_weeks_gen_data, 1)\n",
        "\n",
        "baseline_gen_accLeaks_label = sum_of_specific_column(baseline_gen_data, 2)\n",
        "twleve_weeks_gen_accLeaks_label = sum_of_specific_column(twleve_weeks_gen_data, 2)\n",
        "\n",
        "baseline_gen_urges_label = sum_of_specific_column(baseline_gen_data, 3)\n",
        "twleve_weeks_gen_urges_label = sum_of_specific_column(twleve_weeks_gen_data, 3)\n",
        "\n",
        "baseline_gen_nocturia_label = sum_of_specific_column(baseline_gen_data, 5)\n",
        "twleve_weeks_gen_nocturia_label = sum_of_specific_column(twleve_weeks_gen_data, 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyevMBFlGprU"
      },
      "outputs": [],
      "source": [
        "gen_trips_percentage_change_per_day = [(-(twelve - baseline) / baseline * 100 if baseline != 0 else 0) for twelve, baseline in zip(twleve_weeks_gen_trip_label, baseline_gen_trip_label)]\n",
        "gen_accLeaks_percentage_change_per_day = [(-(twelve - baseline) / baseline * 100 if baseline != 0 else 0) for twelve, baseline in zip(twleve_weeks_gen_accLeaks_label, baseline_gen_accLeaks_label)]\n",
        "gen_urges_percentage_change_per_day = [(-(twelve - baseline) / baseline * 100 if baseline != 0 else 0) for twelve, baseline in zip(twleve_weeks_gen_urges_label, baseline_gen_urges_label)]\n",
        "gen_nocturia_percentage_change_per_day = [(-(twelve - baseline) / baseline * 100 if baseline != 0 else 0) for twelve, baseline in zip(twleve_weeks_gen_nocturia_label, baseline_gen_nocturia_label)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el9B5420A-Ha"
      },
      "outputs": [],
      "source": [
        "def label_numbers(arr, threshold):\n",
        "    labels = []\n",
        "    for i in arr:\n",
        "        if i < -1 * threshold:\n",
        "            labels.append(-1)\n",
        "        elif -1 * threshold <= i <= threshold:\n",
        "            labels.append(0)\n",
        "        else:\n",
        "            labels.append(1)\n",
        "    return labels\n",
        "\n",
        "# def label_numbers(arr, threshold):\n",
        "#     labels = []\n",
        "#     for i in arr:\n",
        "#         if i < threshold:\n",
        "#             labels.append(0)\n",
        "#         else:\n",
        "#             labels.append(1)\n",
        "#     return labels\n",
        "\n",
        "def average_position(arrays):\n",
        "    averages = []\n",
        "    length = len(arrays[0])\n",
        "    for i in range(length):\n",
        "        avg = sum(arr[i] for arr in arrays) / len(arrays)\n",
        "        if avg > 0.5:\n",
        "            label = 1\n",
        "        else:\n",
        "            label = 0\n",
        "        averages.append(label)\n",
        "    return averages\n",
        "\n",
        "trips_label = label_numbers(gen_trips_percentage_change_per_day, np.average(gen_trips_percentage_change_per_day))\n",
        "accLeak_label = label_numbers(gen_accLeaks_percentage_change_per_day, np.average(gen_accLeaks_percentage_change_per_day))\n",
        "urges_label = label_numbers(gen_urges_percentage_change_per_day, np.average(gen_urges_percentage_change_per_day))\n",
        "nocturia_label = label_numbers(gen_nocturia_percentage_change_per_day, np.average(gen_nocturia_percentage_change_per_day))\n",
        "# threshold = 30\n",
        "# trips_label = label_numbers(gen_trips_percentage_change_per_day, threshold)\n",
        "# accLeak_label = label_numbers(gen_accLeaks_percentage_change_per_day, threshold)\n",
        "# urges_label = label_numbers(gen_urges_percentage_change_per_day, threshold)\n",
        "# nocturia_label = label_numbers(gen_nocturia_percentage_change_per_day, threshold)\n",
        "\n",
        "baseline_gen_data_for_train = select_and_transpose_columns(baseline_gen_data, columns)\n",
        "twleve_weeks_gen_data_for_train = select_and_transpose_columns(twleve_weeks_gen_data, columns)\n",
        "t_baseline_gen_data_for_train = [np.concatenate(sublist, axis=1) for sublist in baseline_gen_data_for_train] #prepared data for train\n",
        "t_twleve_weeks_gen_data_for_train = [np.concatenate(sublist, axis=1) for sublist in twleve_weeks_gen_data_for_train] #prepared data for train\n",
        "\n",
        "gen_labels = average_position([trips_label, nocturia_label, accLeak_label, urges_label]) #prepared label for train\n",
        "\n",
        "actual_participant_baseline = [sequence[1:] for sequence in stacked_participant_baseline] #Actual data for test\n",
        "actual_participant_week12 = [sequence[1:] for sequence in stacked_participant_week12] #Actual data for test\n",
        "actual_labels = [1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1] #Actual labels for test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "indices_of_responder = [index for index, value in enumerate(gen_labels) if value == 1]\n",
        "indices_of_non_responder = [index for index, value in enumerate(gen_labels) if value == 0]\n",
        "\n",
        "responder_base_gen_trip_label = [baseline_gen_trip_label[index] for index in indices_of_responder]\n",
        "non_responder_base_gen_trip_label = [baseline_gen_trip_label[index] for index in indices_of_non_responder]\n",
        "responder_twleve_weeks_gen_trip_label = [twleve_weeks_gen_trip_label[index] for index in indices_of_responder]\n",
        "non_responder_twleve_weeks_gen_trip_label = [twleve_weeks_gen_trip_label[index] for index in indices_of_non_responder]\n",
        "\n",
        "responder_baseline_gen_accLeaks_label = [baseline_gen_accLeaks_label[index] for index in indices_of_responder]\n",
        "non_responder_baseline_gen_accLeaks_label = [baseline_gen_accLeaks_label[index] for index in indices_of_non_responder]\n",
        "responder_twleve_weeks_gen_accLeaks_label = [twleve_weeks_gen_accLeaks_label[index] for index in indices_of_responder]\n",
        "non_responder_twleve_weeks_gen_accLeaks_label = [twleve_weeks_gen_accLeaks_label[index] for index in indices_of_non_responder]\n",
        "\n",
        "responder_baseline_gen_urges_label = [baseline_gen_urges_label[index] for index in indices_of_responder]\n",
        "non_responder_baseline_gen_urges_label = [baseline_gen_urges_label[index] for index in indices_of_non_responder]\n",
        "responder_twleve_weeks_gen_urges_label = [twleve_weeks_gen_urges_label[index] for index in indices_of_responder]\n",
        "non_responder_twleve_weeks_gen_urges_label = [twleve_weeks_gen_urges_label[index] for index in indices_of_non_responder]\n",
        "\n",
        "responder_baseline_gen_nocturia_label = [baseline_gen_nocturia_label[index] for index in indices_of_responder]\n",
        "non_responder_baseline_gen_nocturia_label = [baseline_gen_nocturia_label[index] for index in indices_of_non_responder]\n",
        "responder_twleve_weeks_gen_nocturia_label = [twleve_weeks_gen_nocturia_label[index] for index in indices_of_responder]\n",
        "non_responder_twleve_weeks_gen_nocturia_label = [twleve_weeks_gen_nocturia_label[index] for index in indices_of_non_responder]"
      ],
      "metadata": {
        "id": "JAyiHf-p1Haa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i13Zw5ks22KH"
      },
      "outputs": [],
      "source": [
        "def get_svd(matrix):\n",
        "  try:\n",
        "    U, Sigma, Vh = np.linalg.svd(matrix, full_matrices=True)\n",
        "  except Exception:\n",
        "    Sigma = [0]\n",
        "  return sum(Sigma)\n",
        "\n",
        "def get_cond(matrix):\n",
        "  try:\n",
        "    return np.linalg.cond(matrix)\n",
        "  except Exception:\n",
        "    return np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7bKqAb2yk9zv"
      },
      "outputs": [],
      "source": [
        "E2_norm_base = [np.linalg.norm(matrix, 'fro') for matrix in t_baseline_gen_data_for_train]\n",
        "sum_diag_base = [get_svd(matrix) for matrix in t_baseline_gen_data_for_train]\n",
        "E2_norm_12 = [np.linalg.norm(matrix, 'fro') for matrix in t_twleve_weeks_gen_data_for_train]\n",
        "sum_diag_12 = [get_svd(matrix) for matrix in t_twleve_weeks_gen_data_for_train]\n",
        "ranks_base = [matrix.shape[1] for matrix in t_baseline_gen_data_for_train]\n",
        "ranks_12 = [matrix.shape[1] for matrix in t_twleve_weeks_gen_data_for_train]\n",
        "cond_base = [get_cond(matrix) for matrix in t_baseline_gen_data_for_train]\n",
        "cond_12 = [get_cond(matrix) for matrix in t_twleve_weeks_gen_data_for_train]\n",
        "#print(len(t_baseline_gen_data_for_train))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvalues, Autocorrelation Coefficients, Variance, Covariance, Singular Value, Granger Causality Statistic"
      ],
      "metadata": {
        "id": "2YctPBf1SX4I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oYb8tbpak-Fu"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression, LogisticRegression, LassoLars\n",
        "import pandas as pd\n",
        "\n",
        "# df = pd.DataFrame({\"E2_norm_base\": E2_norm_base, \"sum_diag_base\": sum_diag_base, \"E2_norm_12\": E2_norm_12, \"sum_diag_12\": sum_diag_12, \"ranks_base\": ranks_base, \"ranks_12\": ranks_12, \"cond_base\": cond_base, \"cond_12\": cond_12, \"target\": gen_labels})\n",
        "# df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "df = pd.DataFrame({\"E2_norm_base\": E2_norm_base, \"sum_diag_base\": sum_diag_base, \"ranks_base\": ranks_base, \"cond_base\": cond_base, \"target\": gen_labels})\n",
        "df = df.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "\n",
        "# X = df[['E2_norm_base', 'sum_diag_base', \"E2_norm_12\", \"sum_diag_12\", \"ranks_base\", \"ranks_12\", \"cond_base\", \"cond_12\"]]\n",
        "# y = df['target']\n",
        "\n",
        "X = df[['E2_norm_base', 'sum_diag_base', \"ranks_base\", \"cond_base\"]]\n",
        "y = df['target']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_zeros = (y == 0).sum()\n",
        "num_ones = (y == 1).sum()\n",
        "\n",
        "total = len(y)\n",
        "\n",
        "percentage_zeros = (num_zeros / total) * 100\n",
        "percentage_ones = (num_ones / total) * 100\n",
        "\n",
        "print(f\"Number of non-responders (average threshold): {num_zeros} ({percentage_zeros:.2f}%)\")\n",
        "print(f\"Number of responders (average threshold): {num_ones} ({percentage_ones:.2f}%)\")"
      ],
      "metadata": {
        "id": "8IgW-dEH3tbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression()\n",
        "model.fit(X, y)"
      ],
      "metadata": {
        "id": "eTo0cMbn3vDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWwKR-nX3pQK"
      },
      "outputs": [],
      "source": [
        "# Try Lasso\n",
        "\n",
        "from sklearn import linear_model\n",
        "#reg = linear_model.Lasso(alpha=0.01)\n",
        "reg = LassoLars(alpha=.01)\n",
        "reg.fit(X, y)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j_Cyz7Rqk-Mf"
      },
      "outputs": [],
      "source": [
        "E2_norm_test = [np.linalg.norm(matrix, 'fro') for matrix in actual_participant_baseline]\n",
        "sum_diag_test = [get_svd(matrix) for matrix in actual_participant_baseline]\n",
        "E2_norm_12_test = [np.linalg.norm(matrix, 'fro') for matrix in actual_participant_week12]\n",
        "sum_diag_12_test = [get_svd(matrix) for matrix in actual_participant_week12]\n",
        "ranks_base_test = [matrix.shape[1] for matrix in actual_participant_baseline]\n",
        "ranks_12_test = [matrix.shape[1] for matrix in actual_participant_week12]\n",
        "cond_base_test = [get_cond(matrix) for matrix in actual_participant_baseline]\n",
        "cond_12_test = [get_cond(matrix) for matrix in actual_participant_week12]\n",
        "\n",
        "# df_test = pd.DataFrame({'E2_norm_base': E2_norm_test, 'sum_diag_base': sum_diag_test, \"E2_norm_12\": E2_norm_12_test, \"sum_diag_12\":sum_diag_12_test, \"ranks_base\": ranks_base_test, \"ranks_12\": ranks_12_test, \"cond_base\": cond_base_test, \"cond_12\": cond_12_test,\"target\": actual_labels})\n",
        "df_test = pd.DataFrame({'E2_norm_base': E2_norm_test, 'sum_diag_base': sum_diag_test, \"ranks_base\": ranks_base_test, \"cond_base\": cond_base_test, \"target\": actual_labels})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uhCAFoHck-O3"
      },
      "outputs": [],
      "source": [
        "df_test = df_test.replace([np.inf, -np.inf], np.nan).dropna()\n",
        "\n",
        "# X_test = df_test[['E2_norm_base', 'sum_diag_base', \"E2_norm_12\", \"sum_diag_12\", \"ranks_base\", \"ranks_12\", \"cond_base\", \"cond_12\"]]\n",
        "# y_test = df_test['target']\n",
        "\n",
        "X_test = df_test[['E2_norm_base', 'sum_diag_base', \"ranks_base\", \"cond_base\"]]\n",
        "y_test = df_test['target']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TdfRkcXbk-SY"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "print(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNyIsX7m326p"
      },
      "outputs": [],
      "source": [
        "predictions_lasso = reg.predict(X_test)\n",
        "print(predictions_lasso)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xidyqf859dH"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x,threshold=0.5):\n",
        "    sigmoid_values = 1 / (1 + np.exp(-x))\n",
        "    binary_values = (sigmoid_values >= threshold).astype(int)\n",
        "    return binary_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulLHR7_96BJE"
      },
      "outputs": [],
      "source": [
        "predictions_binary = [sigmoid(x) for x in predictions_lasso]\n",
        "#print(predictions_binary)\n",
        "print(f\"accuracy: {1 - sum(np.abs(predictions - y_test))/ len(y_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zsm5kbvzHfK7"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "\n",
        "model = GradientBoostingClassifier(random_state=42)\n",
        "model.fit(X, y)\n",
        "y_pred = model.predict(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUYMXKiWYYDW"
      },
      "outputs": [],
      "source": [
        "print(f\"  accuracy: {accuracy_score(y_test, y_pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXWopo_8ZZQh"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "model = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyEmvqctZkYG"
      },
      "outputs": [],
      "source": [
        "print(f\"  accuracy: {accuracy_score(y_test, y_pred)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhLjB4UP2Sdo"
      },
      "outputs": [],
      "source": [
        "from keras import backend as K\n",
        "\n",
        "def f1_score(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n",
        "    tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n",
        "\n",
        "    precision = tp / (tp + fp + K.epsilon())\n",
        "    recall = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
        "    f1 = K.mean(f1)\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SKZ6UFGt-Au"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "\"\"\"\n",
        "Please uncomment the below two lines if you run from the beginning.\n",
        "\"\"\"\n",
        "X = X.values.reshape((X.shape[0], 1, X.shape[1]))\n",
        "X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(50, activation='relu', return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(LSTM(50, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_score])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mspOFRonuKbm"
      },
      "outputs": [],
      "source": [
        "model.fit(X[:8000], y[:8000], epochs=50, batch_size=32, validation_data=(X[8000:], y[8000:]), verbose=1)\n",
        "model.evaluate(X_test, y_test, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "up5L9OexvDjD"
      },
      "outputs": [],
      "source": [
        "loss, accuracy, f1 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "print(f'Test F1 Score: {f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ryuCqVW9xbkK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, activation='relu', input_shape=(X.shape[1], X.shape[2])))\n",
        "model.add(Dense(1, activation='sigmoid'))  # Sigmoid activation for binary classification\n",
        "\n",
        "# Compile the model with accuracy as a metric\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_score])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_65O-2rxyQe"
      },
      "outputs": [],
      "source": [
        "model.summary()\n",
        "X[:8000].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUyVNrAFx1DZ"
      },
      "outputs": [],
      "source": [
        "model.fit(X[:8000], y[:8000], epochs=50, batch_size=32, validation_data=(X[8000:], y[8000:]), verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vekkaYlYyCKA"
      },
      "outputs": [],
      "source": [
        "loss, accuracy, f1 = model.evaluate(X_test, y_test, verbose=1)\n",
        "print(f'Test Loss: {loss}')\n",
        "print(f'Test Accuracy: {accuracy}')\n",
        "print(f'Test F1 Score: {f1}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wR-lDCWjybQ2"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(\"F1 Score:\", f1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "y_pred = (model.predict(X) > 0.5).astype(\"int32\")\n",
        "cm = confusion_matrix(y, y_pred)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O3ZYe2M5c8Th"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}